#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼“å­˜ç®¡ç†å™¨ - å…¼å®¹åŸå§‹å›æµ‹å¼•æ“çš„ç¼“å­˜ç³»ç»Ÿ
"""

import os
import pickle
import hashlib
from pathlib import Path
from typing import Optional, Any, Dict
import logging
try:
    from core.config import settings
except ImportError:
    from fastapi_backend.core.config import settings

logger = logging.getLogger(__name__)

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - å®Œå…¨å…¼å®¹åŸå§‹ç³»ç»Ÿ"""
    
    def __init__(self, cache_dir: Path = None):
        self.cache_dir = cache_dir or settings.cache_dir
        self.enabled = settings.cache_enabled
        
    def init(self):
        """åˆå§‹åŒ–ç¼“å­˜ç›®å½•"""
        if self.enabled:
            self.cache_dir.mkdir(exist_ok=True)
            logger.info(f"âœ… ç¼“å­˜ç›®å½•åˆå§‹åŒ–: {self.cache_dir}")
            
            # æ£€æŸ¥ç°æœ‰ç¼“å­˜æ–‡ä»¶
            cache_files = list(self.cache_dir.glob("preprocessed_data_*.pkl"))
            logger.info(f"ğŸ“ å‘ç° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
            
            for cache_file in cache_files:
                size_mb = cache_file.stat().st_size / (1024 * 1024)
                logger.info(f"  - {cache_file.name}: {size_mb:.1f} MB")
    
    def get_cache_key(self, data_file: str, start_date: str = None, end_date: str = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”® - ä¸åŸå§‹ç³»ç»Ÿå®Œå…¨å…¼å®¹"""
        if start_date is None and end_date is None:
            key_string = f"{data_file}_FULL_DATASET"
        else:
            key_string = f"{data_file}_{start_date}_{end_date}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def load_cache(self, cache_key: str) -> Optional[Any]:
        """åŠ è½½ç¼“å­˜æ•°æ® - ä¸åŸå§‹ç³»ç»Ÿå®Œå…¨å…¼å®¹"""
        if not self.enabled:
            return None
            
        cache_file = self.cache_dir / f"preprocessed_data_{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                logger.info(f"âœ… ç¼“å­˜å‘½ä¸­: {cache_file.name}")
                return data
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        return None
    
    def save_cache(self, cache_key: str, data: Any):
        """ä¿å­˜ç¼“å­˜æ•°æ® - ä¸åŸå§‹ç³»ç»Ÿå®Œå…¨å…¼å®¹"""
        if not self.enabled:
            return
            
        cache_file = self.cache_dir / f"preprocessed_data_{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            
            size_mb = cache_file.stat().st_size / (1024 * 1024)
            logger.info(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {cache_file.name} ({size_mb:.1f} MB)")
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def load_full_dataset_cache(self, data_file: str) -> Optional[Any]:
        """åŠ è½½å…¨é‡æ•°æ®é›†ç¼“å­˜"""
        cache_key = self.get_cache_key(data_file)
        return self.load_cache(cache_key)
    
    def save_full_dataset_cache(self, data_file: str, data: Any):
        """ä¿å­˜å…¨é‡æ•°æ®é›†ç¼“å­˜"""
        cache_key = self.get_cache_key(data_file)
        self.save_cache(cache_key, data)
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        if not self.enabled:
            return {"enabled": False}
        
        cache_files = list(self.cache_dir.glob("preprocessed_data_*.pkl"))
        total_size = sum(f.stat().st_size for f in cache_files)
        
        return {
            "enabled": True,
            "cache_dir": str(self.cache_dir),
            "file_count": len(cache_files),
            "total_size_mb": total_size / (1024 * 1024),
            "files": [
                {
                    "name": f.name,
                    "size_mb": f.stat().st_size / (1024 * 1024),
                    "modified": f.stat().st_mtime
                }
                for f in cache_files
            ]
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if not self.enabled:
            return
        
        cache_files = list(self.cache_dir.glob("preprocessed_data_*.pkl"))
        for cache_file in cache_files:
            try:
                cache_file.unlink()
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ç¼“å­˜: {cache_file.name}")
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çŠ¶æ€"""
        return {
            "enabled": self.enabled,
            "cache_dir_exists": self.cache_dir.exists() if self.enabled else False,
            "cache_info": self.get_cache_info() if self.enabled else {}
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ ç¼“å­˜ç®¡ç†å™¨æ¸…ç†å®Œæˆ")

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
cache_manager = CacheManager()
